\documentclass{report}

\usepackage{natbib} % Harvard style bib
\usepackage{lipsum}
\usepackage{url}
\usepackage{hyperref}
\usepackage{minted}

\begin{document}

\title{LibrumCI: Leveraging Container Cluster Management Framework Natives in Continuous Integration}

\numberofauthors{1}\author{
\alignauthor
Anthony Troy\\
       \affaddr{School of Computing}\\
       \affaddr{Dublin City University}\\
       \affaddr{Dublin 9, Ireland}\\
       \email{\\anthony.troy3@mail.dcu.ie}
}

\date{22 August 2016}

\maketitle

\begin{abstract}
\lipsum[1-2] 

\end{abstract}

\keywords{Continuous Integration; Kubernetes; Continuous Delivery; Docker; Cluster Management Frameworks; VCS; Agile}

\section{Introduction}
With organisational structures diversifying to off-shore, insource and outsource
development needs, software is increasingly being developed by fully- or 
partially-distributed teams. The contributions from team members must, at a minimum, be first validated
under a test infrastructure that is automated and robust enough to allow for
high levels of code churn. This development requirement is reflected in 
in the sustained and growing adoption of Continuous Integration (CI) practices in open-source and enterprise software projects \citep{Duvall, Fitz, Vas}. 
\par
To-do, mention the requirements and then pitfalls/challenges of CI (expense of provisioning prod-like envs, configuration management, slow feedback). To-do, mention the advent of containerisation and container cluster management frameworks. To-do, mention the lacking CI open-source tooling support for containers. To-do, define container cluster management frameworks and mention how they can remedy CI challenges, CI container support and more. 

%One key innovation
%is the idea of continuous integration (CI); essentially, CI attempts
%to automatically build and deploy the software in a
%?sandbox?, and automatically run a collection of tests when
%the pull request is received. By automating these steps, a
%project can hope to gain both productivity (more pull requests
%accepted) and quality (the accepted pull requests are
%prescreened by the automation provided by CI).
%Starting from a newly mined data set of the usage of CI
%in GitHub projects, in this paper we looked at the software
%engineering outcomes which present di?erentially with
%the introduction of CI versus without. In particular, our
%contributions are:
%? We collected a comprehensive data set of 246 GitHub
%projects which at some point in their history added
%the Travis-CI functionality to the development process.
%Our data is available online at https://github.com/
%yuyue/pullreq_ci.
%? We found that after CI is added, more pull requests
%from core developers are accepted, and fewer are rejected;
%and fewer submissions from non-core developers
%get rejected. This suggests that CI both improves
%the handling of pull requests from insiders, and has an
%overall positive e?ect on the initial quality of outside
%submissions.
%? Despite the increased volume of pull requests accepted,
%we found that introduction of CI is not associated with
%any diminishment of user-reported bugs, thus suggesting
%that user-experienced quality is not negatively affected.
%We did see an increase in developer reported
%bugs, which suggests that CI is helping developers discover

%CI can enable you to release deployable software at any point in time.
%From an outside perspective, this is the most obvious benefit of CI. We
%could talk endlessly about improved software quality and reduced
%risks, but deployable software is the most tangible asset to ?outsiders?
%such as clients or users. The importance of this point cannot be overstated.
%CI provides the ability to notice trends and make effective decisions,
%and it helps provide the courage to innovate new improvements.
%Overall, effective application of CI practices can provide greater confidence
%in producing a software product. With every build, your team
%knows that tests are run against the software to verify behavior, that
%project coding and design standards are met, and that the result is a
%functionally testable product. (CI book)

\section{Related Work}
\lipsum[1] 


\section{Background}
Traditionally most software projects have been characterised as having
poor integration practices. More often than often not a siloed team 
will conduct all development activities, arguably over a lengthy period,
and defer acceptance testing and code integration. Importantly, during
this period one can assert that the software is in an inoperative state, as there has
been no motivation to actually run the software and use it in a
production-like environment.
\par 
\citet{Jez} argue that many projects ``schedule lengthy integration phases at
the end of development to allow the development team time to get the branches
merged and the application working so it can be acceptance-tested". More concerning
is that when some projects arrive at this point ``their software is not fit for
purpose". These long-winded and challenging phases, commonly referred
to as ``integration hell", pose difficulties for the engineers working through the integration
and too for the leads and managers estimating the project delivery.
\par
Applications of Continuous Integration practices have been found to effectively 
remove these phases at source \citep{Vas, Fitz, Jez, Duvall}. As a practice, CI seeks to reduce software deployment lead time by
facilitating frequent code churn through verifying contributions with automated 
builds and tests. Depending on the structure of the team and software, \citet{Jez} 
advise that a team member should integrate their work at least once daily. An effective 
undertaking of this process means that the software is always in a working state. 
\par
To-do, highlight of challenges applying CI (provisioning build farms with things like Puppet, compute expense, configuration management, maintaining production parody). 
To-do, highlight the advent of containerisation (effective smart building, prod build and config parody can be more easily achieved).
To-do, briefly discuss how distributed container technology works (distributed caching of layers and images across nodes)
To-do, discuss production-grade container scheduling and management frameworks vs traditionally build farm technology (Kuberentes vs Jenkins).

\subsection{Continuous Integration}
Practices resembling Continuous Integration could be said
to stem as far back as the original lean manufacturing method. 
Nevertheless in software
engineering the practice was codified by \citet{Beck} 
as a core tenant of the Extreme Programming (XP) method.
Following, CI was also included in the Agile Manifesto as an encouraged
quality assurance practice. Amidst its wide adoption and popularity however, 
there is some variability in how teams are employing the practice, resulting 
is some debate as to exactly which integration activities one should follow \citep{Stahl}.
Nevertheless, at its core, CI forwards several central principles and high-level activities.

\subsubsection{CI Activities}
Before implementing CI a process for a given project, all software source code
and assets must be captured under a version control system (e.g. Git, SVN or Mercurial).
Such a system manages file changes using a controlled access repository. Importantly,
this provides the mechanism for developers to persist and maintain a mainline branch of the working software 
(e.g. the head or trunk branch), this is where any desired contributions are to be frequently 
integrated/merged \citep{Vas}. Version control systems also support reverting back to
previous revisions of the software, which is commonly availed of in CI. 
\par
Central to CI is the mechanism in which integrations are built, tested and fed back to the team. 
More often than not an automated CI system will manage these tasks. Today existing systems
are available as either hosted services or standalone setups \citep{Gous}, and there are
a multitude of such offerings in both the open-source and commercial spaces. 
The most notable of which are Jenkins, TravisCI, CruiseControl, CircleCI and Bamboo. 
As we will later explore, these systems offer varying approaches to how one configures and
defines a build pipeline, nevertheless on an integration build machine or cluster the following workflow generally occurs:
\begin{itemize}  
\item the CI server will poll, or listen to, contributions 
made against the version control repository.
\item upon receiving a contribution, the CI 
server will retrieve the source files and subsequently run build and tests scripts. 
\item the following build results are most commonly made available through a web interface and possibly
email.
\item the built application package, such as a WAR or EXE file, is also made available \citep{Stahl}.
\end{itemize}
\par
Continuously integrating reduces software project risk through 
the preventative deciton of defects by in turn reducing quality 
assumptions, and by eliminating manual repetitive integration
processes ``time, costs and effort" can be saved  \citep{Vas}.
CI also affords the software being in a deployable state at any time \citep{Jez}.
Indeed, as a process CI is quite arguably beneficial, nevertheless challenges 
do exist around implementing a robust CI system 
and encompassing process.

\subsubsection{CI Challenges}
Employing an effective configuration management strategy is perhaps one of the
most challenging aspects of CI. It involves ensuring that every application-level configuration
needed per build phase (e.g. build, deploy and test) is captured and managed under one's
version control system. The same is required for the configurations around one's entire
production infrastructure, from the patch-level of an operating system (OS)
to the configuration files of databases and load-balancers. Furthermore, all of 
the dependencies between artefacts should also be expressed. \cite{Jez}
define this as ``the process by which all artefacts relevant to
your project, and the relationships between them, are stored, retrieved, uniquely
identified, and modified". Codifying the entire state of an infrastructure in
version control is certainly not trivial for most projects, particularly for non-greenfield projects.
\par
\cite{Jez} argue that a successful configuration strategy requires a ``holistic approach to 
managing all infrastructure". This involves the ``desired state", or current configuration, of 
one's infrastructure being tracked under version control. A major benefit of keeping
absolutely everything in version control is that source 
changes made at any level will trigger a CI build, which is in turn integrated, tested and deployable. 
That includes DBAs updating SQL scripts and operations members changing firewall 
rules. However, supporting this requires implementing automated infrastructure provisioning.
For instance, bumping the OS version which serves the application software
must trigger a CI build, but first this must cause 
the OS to be re-provisioned on the CI server and the rest of the downward dependencies 
to be rebuilt (e.g. OS libraries, middleware, application build script).  Tools like Chef, Puppet 
and Ansible are established automation tools in this space however, as we will later discuss, 
we argue that such heavyweight solutions are less well placed in modern CI practices.
\par
True for both hosted services and standalone setups, powerful servers or build farms usually 
underly a CI system to ensure that build-times are as a quick as possible \citep{Campos}. As a consequence,
these shared remote resources ``become a bottleneck when the number
of developers increases" as more concurrent builds can occur \citep{Gambi}. Several practices have
been adopted to alleviate these limitations. One addresses this horizontally through the ``componentisation of larger
applications", wherein a codebase is divided into well-defined parts that are developed 
and integrated separately. Thus, resulting in separate CI builds for each component \citep{Jez}.
Other approaches emphasise on test selection. For instance, those tests related to the changes made in a given contribution
are ran first (``pre-submit"), and if successful the integration is made, then the remaining tests are executed (``post-submit") and if a failure occurs the integration is rolled back \citep{Elbaum}.


\subsubsection{CI in Practice}
Industry studies have found that 57\% of companies are employing
CI on a daily or weekly basis\footnote{\href{http://info.thoughtworks.com/rs/thoughtworks2/images/Continuous\%20Delivery\%20_\%20A\%20Maturity\%20Assessment\%20ModelFINAL.pdf}{http://info.thoughtworks.com/rs/thoughtworks2/images/\\Continuous\%20Delivery\%20_\%20A\%20\\Maturity\%20Assessment\%20ModelFINAL.pdf}}, however there is a clear disparity in how those
practitioners are actually implementing the process. \citet{Stahl} have revealed that there are several disagreements in the existing research around how CI is actually practiced in industry. 
\par
CI has also become a well adopted practice in open-source projects,
particularly for those projects (75\%) hosted on GitHub. Those surveyed 
integrators have been found to use ``few quality evaluation tools other than
continuous integration" in their integration workflows \citep{Gous}. 
In the case of GitHub hosted projects, 
TravisCI is currently the most popular tool \citep{Yu}. Similar to many other
modern hosted CI solutions, a repository is primarily configured 
with the system through a repository-level YAML file that declares the entire build workflow  \citep{Santos}.
This file includes the infrastructural configuration (e.g. host OS type, host OS version, middleware dependancies), the application-level configuration, and finally the build and test scripts to be ran. The following \texttt{.travis-ci.yml} file is quite
similar to that used on the Ruby on Rails project:
\begin{minted}[
    frame=single
  ]{yaml}
# use a Linux image with Ruby pre-installed
os: linux 
language: ruby
sudo: false

cache:
  bundler: true
  directories:
    - # between builds cache installed `bundler' 
    - # packages (gems) and any other directories
before_install:
  - # before starting services and add-ons fetch
  - # and configure other system and/or 
  - # middleware dependancies
# ensure services are running and available 
# over the network
services:
  - memcached
  - redis
  - rabbitmq
addons: # use beta services
  postgresql: ``9.4"
before_script:
  - # before running tests run some commands 
  - # like (re)configuring the application
script: 
  - ci/run_tests.rb # test script to run
  - ci/run_inter.rb # integration script to run
# use each Ruby versions against the test script
rvm:
- 2.2.5
- ruby-head
# run the test script again under each specified 
# environment variable 
env:
  matrix:
    - `SOME_FLAG=0'
    - `SOME_FLAG=1'
notifications:
  - # send builds statuses to IRC
  \end{minted}
\par
Upon receiving a contribution, such a service will provision
and build a virtual environment based on the repository's CI configuration
file, cache those whitelisted directories for subsequent builds, then and run any
specified scripts or commands (which are generally for testing and integrating).
Most services offer a web interface wherein build statuses and be observed, and 
integrations with other tools for notifying build statuses. As you would expect,
a build failure occurs when a provisioning-, test- or integration-related command
terminates unsuccessfully. 
\par
In theory, for most commercial software projects it is impossible to
employ a mature CI process under such tools. The execution environments offered by such services are generally limiting. 
For instance, TravisCI supports  only three virtual environments for build execution: Ubuntu 12.04,
OS X Mavericks and Ubuntu 14.04. CI under these constraints may be
adequate for software projects that only rely on unit- and functional- level
testing. However, as we will later explore, more complex ``n-tierd" applications require build pipelines that 
include acceptance testing on an environment akin to production. 
\par
Standalone setup CI systems offer greater flexibility around build 
execution environments. For instance, under Jenkins one 
can configure a build pipeline which automatically provisions
build execution environments. Such a pipeline might involve
running unit and functional tests in a smaller production-like environment, and
upon success run acceptance tests in a production sandbox environment. Thus,
truly increasing confidence in the integrated contribution and ensuring that
the software is always in a working state. 

\subsection{Containerisation}
Selecting appropriate automation tools
generally comes down to what is ``best fit for your
environment and development process". \citet{Duvall}
argues that this includes the functionality, reliability, longevity and 
usability of the given tool. All too often these
evaluations ``often transcend the practical and 
escalate into what sounds like a religious debate". 
As we will subsequently explore, recently established 
standards and technologies in containerisation 
afford significant opportunities to CI.

\subsubsection{Establishing the Container Standard}
Containerisation is a recently resurged computing paradigm that is
having a significant impact on how applications are being built,
shipped and ran. Along with being less resource intensive and
more portable, containers simplify dependency management, application
versioning and service scaling, as opposed to deploying
applications or application components directly onto a host operating
system. Docker, albeit a relatively young project, has successfully
established a container standard.
\par
Containers have a long history in computing though much of their recent popularity 
surround the recent developments of both LXC and the Docker platform. 
The former can be described as a container execution environment,
or more formally, a Linux user space interface to 
access new kernel capabilities of achieving process isolation through namespaces
and cgroups \citep{Claus}. The latter is an open-source suite of tools managed by Docker Inc.\ which
extends upon container technology such as LXC, in turn 
allowing containers to behave like ``full-blown hosts in their own right" 
whereby containers have ``strong isolation, their own network and storage stacks, as well 
as resource management capabilities to allow friendly co-existence of multiple containers on a host" \citep{db}.
\par 
Uncertainties around Docker's maturity and production-readiness have been expressed \citep{Kereki, Powers, Merkel}, however 
over the last two years the states of both Docker and the containerisation ecosystem continue to rapidly progress.\
Last year Docker has seen an unprecedented increase in development, adoption and community uptake \citep{Merkel}. Most
notably was the introduction of customisable container execution environments. This means as opposed to LXC one can
``take advantage of the numerous isolation tools available" such as ``OpenVZ, systemd-nspawn, libvirt-sandbox, qemu/kvm, BSD Jails and Solaris Zones".
Also included in this 0.9 release was the new built-in container execution driver ``libcontainer", which replaced LXC as the default driver.
Going forward on all platforms Docker can now execute kernel features such as ``namespaces, control groups, capabilities, apparmor profiles, 
network interfaces and firewalling rules" predictably ``without depending on LXC" as an external dependency \citep{Hykes}. 
\par
Interestingly, libcontainer itself was the first project to provide a standard interface for making containers and managing their lifecycle.\
Subsequently the Docker CEO  announced the coming together of industry leaders and others in partnership with the Linux Foundation
to form a ``minimalist, non-profit, openly governed project" named The Open Container Initiative (OCI), with the purpose of defining 
``common specifications around container format and runtime" \citep{Golub}. 
Thereafter Docker donated its base container format and runtime, libcontainer, to be maintained by the OCI. 

\subsubsection{Supporting `N'-Tiered Applications}
Amidst establishing a container standard, Docker has made significant headway in 
supporting multi-host cloud production environments. In terms of native tooling, over the last two years Docker has implemented
a suite of tools for provisioning and orchestrating containers:
\begin{itemize}
\item \textbf{Docker Machine} allows one to provision Docker hosts, which are simply Linux virtual machines (VMs) supporting Docker, on a local machine or cloud. 
Its pluggable driver API currently supports ``provisioning Docker locally with Virtualbox as well as remotely" on cloud providers such Digital Ocean, AWS, Azure and VMware.
\item \textbf{Docker Swarm} is a clustering solution which takes the standard 
``Docker Engine and extends it to enable you to work on a cluster of containers". 
This in turn allows one to ``manage a resource pool of Docker hosts and schedule
containers to run transparently on top, automatically managing workload and providing failover services".
\item \textbf{Docker Compose} is the ``glue" allowing one to compose a multi-host application on top of a Swarm cluster whereby you
can specify how each application is to be ran in the the cluster, in turn allowing one to orchestrate and choreograph local or cloud containers.
\end{itemize}
\noindent In many cases an existing cloud infrastructure depends upon one or more orchestration tools, for example 
Consul for service discovery. Typically, such tools cannot be migrated away from easily and in turn cause ``vendor lock-in".
Consequently, Docker have implemented this trio of orchestration tools in a generic way, 
providing ``a standard interface to service providers so that they can almost be used as plug-and-play solutions" on top of the Docker platform \citep{holla}.

\subsubsection{Applicability in CI}
For a given software application or component, Docker allows
one codify most of all potential environmental configuration
into one manifest or declaration, a \texttt{Dockerfile}. Apart from the
hardware resource requirements of the application (cpu, memory and disk), 
herein one can capture all system- and application- level configurations. Thus,
in CI terms, it effectively acts as a standardised built 
script also capable of specifying target OS type and version.
Consider that, for unit and functional testing only, we require a build execution environment
for a NodeJS application running on Ubuntu Xenial, where ``\texttt{runTests.sh *}" executes our test suites.
Such a \texttt{Dockerfile} would be as follows:
\begin{minted}[
    frame=single
  ]{docker}
FROM ubuntu:16.04 
ENV NPM_CONFIG_LOGLEVEL info
RUN apt-get update
RUN apt-get install -y apt-utils curl git
RUN curl -sL deb.nodesource.com/setup_6.x \
  | bash -
RUN apt-get install -y nodejs 
RUN rm -rf /var/lib/apt/lists/* \
  && apt-get purge -y --auto-remove apt-utils curl
WORKDIR /app
COPY . /app/
RUN npm install
RUN chmod +x /app/runTests.sh
EXPOSE 8080
ENTRYPOINT ["/app/runTests.sh"]
CMD ["*"]
\end{minted}
\par 
To-do, mention Docker's underlying use of filesystems and how this 
makes builds layered, faster and cachable. To-do, mention how
starting a container is extremely fast compared to VMs. To-do, 
mention how n-tiered application acceptance testing may be
supported (docker-compose running separate services) and lead into next section.

%\begin{minted}[
%    frame=single
%  ]{bash}
%docker build -t test-app . \
% && docker run tests-app
%\end{minted}


%The container image, created and leveraged by Docker, 
%is a manifest container defines how to create and run image. 
%
%An image is a filesystem and parameters to use at runtime. It doesn?t have state and never changes. A container is a running instance of an imag
%
%
%Another important Docker feature is the manner in which
%filesystems for the isolated processes are managed. On bootstrapping,
%each container has its own replica of a root filesystem
%defined by a Docker image. Harnessing layered and versioning
%copy-on-write filesystems (AuFS or btrfs), changed
%parts of the filesystem (diffs) are stored alongside a copy of
%the corresponding filesystem parts before destructive operations,
%allowing to restore any filesystem state during the
%lifecycle of a container and to trace the history of write operations.
%
%
%%As previously highlighted, one challenged faced in implementing a robust
%%CI system is facilitating for the automated provisioning of build 
%%execution environments. Given a host machine running Docker engine (the server
%%daemon allowing clients manage containers), one can simply build an 
%%implementation of 
%
%
%
%Each of these states of the filesystem can be tagged
%and reused as a new image, allowing new containers to start
%their lifecycle with that exact filesystem state.
%In addition to the aforementioned prototypical filesystem
%tree, a Docker image groups information about which root
%process resp. entry point to invoke, potentially required adjustment
%to the execution environment of the process (i. e.
%the working directory and environment variables) and which
%resource limits must be respected. The preferred way to create
%an image is def ining aforementioned details in a Dockerfile5
%. This Dockerile also allows to declare process invocations
%and to add additional iles to the base image to adjust
%aspects of the coniguration in the base image. Dockeriles
%also have the advantage of an even more transparent formal
%description of steps to obtain a desired initial coniguration
%environment for processes to be run in containers.
%With volumes Docker provides the possibility to mount
%directories of the host ilesystem or other containers into
%the ilesystem of a container6
%, exempt from the versioning
%by the copy-on-write ilesystem in favour of increased I/O
%performance. Each container receives its own virtual network
%interface, allowing it to connect to other nodes on the
%internal virtual network and to the Internet through a host
%bridge. With the EXPOSE keyword a container can select
%which ports are available to the host system. To provide
%services via network connections from within a container
%to other containers, Docker ofers a linking mechanism7
%.
%Using links, stable host names and ports can be achieved
%for inter-container network communication, further environment
%variables from upstream containers are made available
%as well. It provides a simple but efective way to interchange
%small pieces of information required for successful orchestration.
%Container-based operating system virtualisation has the
%advantage of keeping all dependencies necessary to run an
%application together while major parts (hardware and operating
%system kernel) are shared between instances. We
%expect that the main technical setup for users of this project
%will be commodity hardware with moderate performance
%characteristics. In such a context, the desired orchestration
%of numerous micro services in isolated environments
%with a complete full- or para-virtualised system for each service
%would be prohibitively taxing on limited computation
%power

\subsection{Container Cluster Management}
To-do, introduction to tie in CI and Docker. To-do, reference pains of provisioning CI 'production-like' clusters. Practitioners and industry experts note that cluster management tooling supporting Docker
vary greatly in terms of capability, architecture and target cluster proportion
\citep{goasguen, holla}. This is unsurprising when we consider that all infrastructures 
are not subject to same orchestration requirements and software release cycles.
For instance, slow moving infrastructures can be characterised as having infrequent application deployments,
 hard-coded service configurations and rare service failures which may not have an urgent impact. In contrast, more fast moving infrastructures feature continuous deployments and strong automation in terms of service configuration and recovery. 

\subsubsection{Service orchestration}
Central to cloud cluster management is the ability to elastically provision and tear down clusters. Many cloud providers have introduced their own service orchestration tools such as CloudFormation from AWS and Heat by OpenStack \citep{Dudouet}. 
On a high-level, these tools simply define a cluster template which can be later orchestrated with possibly extended configurations. As previously mentioned, the native Docker orchestration tools support similar features that can clusterise multi-host containers. Docker Compose conceptually defines a similar template to that of Amazon's CloudFormation and allows one to perform orchestration tasks such as provisioning, destroying and scaling on per container basis.
\par
\citet{Claus} describe container-based clusters as consisting of several hosts which are ``virtual servers on hypervisors or possibly bare-metal servers", each of which typically runs several containers that are responsible for scheduling, load balancing and serving an application or service. Meaning containers can be distributed across one or more host machines wherein these hosts might be virtual servers running other services that must also be orchestrated.
\par
Slow moving infrastructures may not be availing of their provider's orchestration tools as doing so is simply not required. Clusters themselves are manually defined once and the scaling of nodes can be introduced during deployments or at scheduled downtime. Nevertheless, Docker Compose supports this manual workflow. Conversely, fast moving infrastructures profit from their provider's orchestration tools, leveraging them to automate tasks around cluster management.\ As discussed previously, Swarm is a native Docker clustering tool for containers which pools Docker engines together into a single virtual host. In conjunction with Docker Compose, it facilitates for transparent orchestration across container clusters. \citep{holla}.
\par
Cluster management frameworks aim to abstract and automate service orchestration activities such as provisioning, scaling, task scheduling, resource utilisation management and failover recovery. Some cloud providers have implemented such frameworks which sit on top of Swarm. For example, Amazon's EC2 Container Service (ECS) is one that uses a shared-state scheduling model to execute tasks on containerised EC2 instances via containers. Each host instance has a preinstalled ECS agent which allows clusterised containers communicate together and with the ECS console. Consequently, via scheduled tasks, ECS clusters can be transparently and dynamically orchestrated.
\par
Stand-alone Swarm or ECS may be fitting orchestration solutions for fast moving infrastructures, however larger-scale clouds that host hundreds or thousands of containers require high-level cluster management platforms such as Apache Mesos and Kubernetes. The former abstracts ``distributed hardware resources into
a single pool of resources" and can provide similar cluster management facilities to ECS when integrated with scheduling and service management tools such as Marathon. The later is a higher-level platform specifically designed for managing containerised applications across multiple hosts including mechanisms for service deployment, scaling and maintenance.

\subsubsection{Service discovery and configuration}
Service discovery and configuration management are central cluster management concepts in distributed systems and microservices-based architectures. Both of which are argued to overlap in nature. Service discovery can be described as an approach to achieve ``dynamic and automatic software system composition, configuration and adaptation" \citep{Yang}. Generally, service discovery implementations accomplish this by allowing application components/services discover information or configurations about their current and  neighbouring environments through a distributed key-value store.
\par
Whether operating under a fast or slow moving infrastructure, requiring a service discovery solution is generally related to having a service-orientated  architecture style. The more distributed a system becomes, the more regularly do services require information about their own and neighbouring environments. The tooling around service discovery ranges in terms of complexity and provided features. DNS (Domain Name Systems) is a well-known and commonly understood standard which allows us ``associate a name with the IP address of one or machines" where the name becomes an ``entry point to the IP address of the host running that service" \citep{Newman}. More advanced tools like Consul and Apache Zookeeper support both configuration management and service discovery. The former is designed specifically for service discovery and can use service health checking features to route traffic away from unhealthy nodes. The later is used for wider variety of cases such ``configuration management, synchronizing data between services, leader election, message queues and as a naming service" \citep{Newman}.
\par 
Container-based service discovery involves the ability to dynamically register and discover multi-host containers among their peers. \citet{holla} poses two techniques to accomplish discovery in Docker; integrating Swarm backend discovery tools or using default Docker features like names and links. Docker Swarm implements a hosted discovery service which uses generated tokens to discover cluster nodes. Being primarily concerned with orchestration, Swarm does not currently support dynamic service registration and configuration. However, to dynamically configure and manage the services in your containers one can use a discovery backend with Swarm such as Etcd, Consul or Zookeeper. 
\par
As previously highlighted, Docker Compose provides a mechanism to link named containers on the same host. This is accomplished by ``inserting the first container's IP address in /etc/hosts when starting the second container". Importantly, the IP address of a container living on a different host ``is not known by the docker daemon running in the current host". The ambassador container pattern achieves cross-host container linking between provider and consumer containers by dynamically configuring network connections through respective intermediate ambassador containers \citep{holla}.

\subsubsection{Kubernetes}
To-do, mention project history. To-do, mention the cluster topology it forwards. To-do, feature run-down. To-do, mention some examples of applications of kubernetes as a build farm and/or in testing.
\lipsum[1] 

\section{Design and Evaluation}
To-do, note the poor container/Docker in TravisCI and Jenkins support. To-do, describe LibrumCI system some detail (architecture, screenshots, etc). To-do, matrix of tool comparison.

%Creating an automated process for deploying infrastructural changes from
%version control is at the core of good change management. The most effective
%way to do this is to require all changes to be made to your environments via a
%central system. Use a testing environment to work out the change you want to
%make, test it in a fresh, production-like staging environment, put it into configuration
%management so that future rebuilds incorporate it, have it approved, and
%then have the automated system roll out the change. Many organizations have
%built their own solutions to this problem, but if you do not have one, you can
%use a data center automation tool like Puppet, CfEngine, BladeLogic, Tivoli, or
%HP Operations Center.
%
%
%Puppet and CfEngine are two examples of tools that make it possible to manage
%operating system configuration in an automated fashion. Using these tools, you
%can declaratively define things such as which users should have access to your
%boxes and what software should be installed. These definitions can be stored in
%your version control system. Agents running on your systems regularly pull the
%latest configuration and update the operating system and the software installed
%on it. With systems like these, there is no reason to log into a box to make fixes:
%All changes can be initiated through the version control system, so you have a
%complete record of every change?when it was made and by whom.
%Virtualization can also improve the efficiency of the environment management
%process. Instead of creating a new environment from scratch using an automated
%process, you can simply take a copy of each box in your environment and store
%it as a baseline. Then it is trivial to create new environments?it can be done by
%clicking a button. Virtualization has other benefits, such as the ability to consolidate
%hardware and to standardize your hardware platform even if your
%applications require heterogeneous environments.
%
%This chapter describes how to keep your application releasable at all times, despite
%being under constant change. One of the key techniques for this is componentization
%of larger applications, so we will treat componentization, including
%building and managing large projects with multiple components, at length.
%What is a component? This is a horribly overloaded term in software, so we
%will try to make it as clear as possible what we mean by it. When we talk about
%components, we mean a reasonably large-scale code structure within an application,
%with a well-defined API, that could potentially be swapped out for another
%implementation. A component-based software system is distinguished by the fact
%that the codebase is divided into discrete pieces that provide behavior through
%well-defined, limited interactions with other components.
%The antithesis of a component-based system is a monolithic system with no
%clear boundaries or separation of concerns between elements responsible for
%different tasks. Monolithic systems typically have poor encapsulation, and tight
%coupling between logically independent structures breaks the Law of Demeter.
%The language and technology are unimportant?it has nothing to do with GUI
%widgets in Visual Basic or Java. Some people call components ?modules.?
% Employing a component-based design is often described as encouraging reuse
%and good architectural properties such as loose coupling. This is true, but it also
%has another important benefit: It is one of the most efficient ways for large teams
%of developers to collaborate. In this chapter, we also describe how to create and
%manage build systems for component-based applications.
%If you work on a small project, you may be thinking of skipping this chapter
%after reading the next section (which you should read regardless of the project
%size). Many projects are fine with a single version control repository and a simple
%deployment pipeline. However, many projects have evolved into an unmaintainable
%morass of code because nobody made the decision to create discrete components
%when it was cheap to do so. The point at which small projects change into
%larger ones is fluid and will sneak up on you. Once a project passes a certain
%threshold, it is very expensive to change the code in this way. Few project leaders
%will have the audacity to ask their team to stop development for long enough to
%rearchitect a large application into components.
% 
% 
%ensure that your team can develop
%as efficiently as possible, while keeping your application always in a releasable
%state. As usual, the principle is to ensure that teams get fast feedback on the effect
%of their changes on the production-readiness of the application. One strategy for
%meeting this goal is to ensure every change is broken down into small, incremental
%steps which are checked into mainline. Another is to break your application down
%into components.
%Dividing an application into a collection of loosely coupled, well-encapsulated,
%collaborating components is not only good design. It allows for more efficient
%collaboration and faster feedback when working on large systems. Until
%your application gets sufficiently large, there is no need to build your components
%individually?the simplest thing is to have a single pipeline that builds your whole
%application at once as the first stage. If you concentrate on efficient commit builds
%and fast unit testing, and implement build grids for acceptance testing, your
%project can grow to a much larger degree that you might think possible. A team
%of up to 20 people working full-time for a couple of years should not need to
%create multiple build pipelines, although of course they should still separate their
%application into components.
%Once you exceed these limits, though, the use of components, dependencybased
%build pipelines, and effective artifact management are the key to efficient
%delivery and fast feedback. The beauty of the approach described in this chapter
%is that it builds on the already beneficial practice of component-based design.
%This approach avoids the use of complex branching strategies, which usually
%leads to serious problems in integrating your application. However, it does depend
%on having a well-designed application that is amenable to a componentized build.
%Unfortunately, we have seen too many large applications that cannot be easily
%componentized in this way. It is very hard to coax such an application into a
%state where it can be easily modified and integrated. So, make sure that you are
%using your technology?s toolchain effectively to write code that can be built as a
%set of independent components once it gets large enough.

%Some address this problem by defining techniques that
%filter and prioritize the tests scheduled for execution such
%that developers could get interesting results within acceptable
%time also in the presence of resource shortage [1]. Others
%instead leverage cloud platforms to access increasingly large
%and elastic pools of computing resources to reduce the risk of
%incurring in bottlenecks [2].
%Current solutions mostly focus on improving the efficiency
%of cloud-based continuous integration by automating repetitive
%activities to setup and execute integration and system tests [3],
%which account for the coordinated deployment of several
%virtual machine instances and their configuration by installing
%software components, restoring system state, and configuring
%test drivers. As an example, system testing of a two-tiered
%Web service requires at least the deployment of two virtual
%machines, the installation of the application server code and
%the business logic on the ?front-end? server, and the installation
%of the database server and its content in the ?back-end? server.
%This setup process repeats for all the instances that are
%started by the tests; therefore, automated solutions have the
%potential of speeding up the overall test execution. However,
%blind automation in the cloud might result in surprisingly
%high costs and long execution times that can easily overpass
%the potential benefits of automation and jeopardize the use
%of cloud-based continuous integration environments. In fact,
%cloud providers charge the usage of any resource, including
%network communications, and the set up of virtual machines
%might involve the download of large amount of data and the
%re-installation of software components whose costs accumulate
%over test executions.(Poster)

\lipsum[1] 

\section{Conclusions}
\lipsum[1-2] 

\vspace{-7.5mm}
\renewcommand{\refname}{\section{References}}
\bibliographystyle{IEEEtranN}
\bibliography{report}
\end{document}
